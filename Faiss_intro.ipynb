{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "import requests\n",
    "from io import StringIO\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # res = requests.get('https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/sick2014/SICK_train.txt')\n",
    "# import json\n",
    "  \n",
    "# # Opening JSON file\n",
    "# f = open('./BigData.json')\n",
    "  \n",
    "# # returns JSON object as \n",
    "# # a dictionary\n",
    "# res = json.load(f)\n",
    "  \n",
    "# # Iterating through the json\n",
    "# # list\n",
    "# # for i in res:\n",
    "# #     print(i)\n",
    "  \n",
    "# # Closing file\n",
    "# f.close()\n",
    "# # text = res\n",
    "# # text[:100]\n",
    "# print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need this in a dataframe, which we build from the `text` string like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticket</th>\n",
       "      <th>next action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'Name': 'I-00000001', 'Notice Level': 'Middle...</td>\n",
       "      <td>[step 1: Update Notice Level to Middle, step 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'Name': 'I-00000002', 'Notice Level': 'High',...</td>\n",
       "      <td>[step 1: Update Notice Level to High, step 2: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'Name': 'I-00000003', 'Notice Level': 'Low', ...</td>\n",
       "      <td>[step 1: Update Notice Level to Low, step 2: C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'Name': 'I-00000004', 'Notice Level': 'Middle...</td>\n",
       "      <td>[step 1: Update Notice Level to Middle, step 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'Name': 'I-00000094', 'Notice Level': 'High',...</td>\n",
       "      <td>[step 1: Update Notice Level to High, step 2: ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              ticket  \\\n",
       "0  {'Name': 'I-00000001', 'Notice Level': 'Middle...   \n",
       "1  {'Name': 'I-00000002', 'Notice Level': 'High',...   \n",
       "2  {'Name': 'I-00000003', 'Notice Level': 'Low', ...   \n",
       "3  {'Name': 'I-00000004', 'Notice Level': 'Middle...   \n",
       "4  {'Name': 'I-00000094', 'Notice Level': 'High',...   \n",
       "\n",
       "                                         next action  \n",
       "0  [step 1: Update Notice Level to Middle, step 2...  \n",
       "1  [step 1: Update Notice Level to High, step 2: ...  \n",
       "2  [step 1: Update Notice Level to Low, step 2: C...  \n",
       "3  [step 1: Update Notice Level to Middle, step 2...  \n",
       "4  [step 1: Update Notice Level to High, step 2: ...  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_json('./BigData.json')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take all samples from `sentence_A` and build sentence embeddings for each - which we can then store in FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Detected suspicious network activity from an internal IP address.', 'Received a report of a data breach involving customer information.', \"Identified a vulnerability in the company's web application.\", 'Received multiple reports of phishing emails targeting employees.', \"Received reports of a possible compromise of an executive's email account.\", 'Detected a potential phishing attempt targeting employees via email.', \"Received reports of a possible compromise of the company's file sharing system.\", \"Identified a critical vulnerability in the company's database management system.\", 'Received reports of a suspicious physical device connected to a workstation.', \"Detected suspicious activity in the company's network traffic logs.\"]\n"
     ]
    }
   ],
   "source": [
    "# sentences = data['ticket']['ticketDetail'].tolist()\n",
    "# sentences[:5]\n",
    "sentences = [] \n",
    "for i in data['ticket']:\n",
    "    sentences.append(i['ticketDetail']) \n",
    "print(sentences) \n",
    "\n",
    "# remove duplicates and NaN\n",
    "sentences = [\n",
    "    sentence.replace('\\n', '') for sentence in list(set(sentences)) if type(sentence) is str\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentences.txt', 'w') as fp:\n",
    "    fp.write('\\n'.join(sentences))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 14.5K *unique* sentences, a much better size. We'll go ahead and build the sentence embeddings (this can take some time, feel free to download the embeddings from [here]()). TODO add link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 768)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "sentence_embeddings = model.encode(sentences)\n",
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save/load from file in the case of needing to reload the notebook for any reason later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./sim_sentences/embeddings_X.npy', 'wb') as fp:\n",
    "    np.save(fp, sentence_embeddings[0:256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_0.npy | 0 -> 11\n"
     ]
    }
   ],
   "source": [
    "# saving data\n",
    "split = 256\n",
    "file_count = 0\n",
    "for i in range(0, sentence_embeddings.shape[0], split):\n",
    "    end = i + split\n",
    "    if end > sentence_embeddings.shape[0] + 1:\n",
    "        end = sentence_embeddings.shape[0] + 1\n",
    "    file_count = '0' + str(file_count) if file_count < 0 else str(file_count)\n",
    "    with open(f'./sim_sentences/embeddings_{file_count}.npy', 'wb') as fp:\n",
    "        np.save(fp, sentence_embeddings[i:end, :])\n",
    "    print(f\"embeddings_{file_count}.npy | {i} -> {end}\")\n",
    "    file_count = int(file_count) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We setup our FAISS database dimensionality (number of dimensions per vector) based on these vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = sentence_embeddings.shape[1]\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flat L2 Index\n",
    "\n",
    "We initialize the flat L2 distance index `IndexFlatL2`, all we need is the specify the vector dimensionality - which in this case is `d == 768` (to align with the sentence-BERT model output embeddings of size `768`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, we will use indexes that require us to `train` them on our data before being used (if we are grouping or transforming the data in any way). `IndexFlatL2` however, is a simple operation and only requires that we calculate distances between vectors when we introduce our query vector `xq` during search. So, in this case, no training is required - which we can confirm by checking the `is_trained` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.is_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so once we're happy that our index is prepared, we then add new vectors using the `add` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.add(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.ntotal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then search given a query `xq` and number of nearest neigbors to return `k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "xq = model.encode([\"Received reports of a possible compromise of an executive's email account.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 7 9 3]] [[3.3633932e-11 8.8316727e+01 1.0385466e+02 1.5351736e+02]]\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 26.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "D, I = index.search(xq, k)  # search\n",
    "print(I, D)  # k-nearest neigbors of the query vector | nprobe == 1: 6495 26392 61709 49932 | nprobe == 10: 36245  6495 57489  8705"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're returning indices `7460`, `10940`, `3781`, and `5747`, which returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"1: Received reports of a possible compromise of an executive's email account.\",\n",
       " \"7: Received reports of a possible compromise of the company's file sharing system.\",\n",
       " '9: Detected a potential phishing attempt targeting employees via email.',\n",
       " '3: Received a report of a data breach involving customer information.']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f'{i}: {sentences[i]}' for i in I[0]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly we have some good matches, everything returned includes people running with a football, or on the context of a football match. Now, if we'd rather extract the numerical vectors from FAISS, we can do that too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = np.zeros((k, d))\n",
    "for i, val in enumerate(I[0].tolist()):\n",
    "    vecs[i, :] = index.reconstruct(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 768)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.50518423,  0.48758391,  0.66647851,  0.03475399,  0.37630296,\n",
       "       -0.11395732,  0.77897424,  0.37778905,  0.26803911, -0.15433937,\n",
       "       -0.09677369,  0.2091168 ,  0.42357036,  0.61092204, -0.77611792,\n",
       "       -0.35400027, -0.88501906,  0.38061816,  0.5002628 , -0.20286959,\n",
       "       -0.40612507, -0.10444601, -0.46866933,  0.07575669,  0.55922788,\n",
       "       -0.02680862,  0.05735995, -0.63364196,  0.10851849,  0.40746295,\n",
       "       -0.43443438, -0.24623522,  0.11907054, -0.65765905, -0.11376578,\n",
       "       -0.57217783,  0.18011299,  0.48327035, -0.36807847,  0.27443936,\n",
       "       -0.76391739,  0.58233398,  0.71162719,  1.09151471, -0.81575686,\n",
       "       -0.87680387, -0.05219297,  0.79401863,  0.16443107, -0.71088141,\n",
       "       -0.85350603,  0.70048892,  1.17236733,  0.51407832, -0.95886159,\n",
       "        0.50808734,  0.14186063, -1.46503985, -0.72266203,  0.70568979,\n",
       "       -0.94487143, -0.68937463,  0.99473774,  0.51572955, -1.02725387,\n",
       "        0.39197078,  0.11724776, -0.0588825 , -0.63245457, -0.53919286,\n",
       "        0.39932927, -1.1048944 , -0.58722752, -0.22883217, -0.58406854,\n",
       "        0.16543522, -0.42372313,  0.67060494,  1.45077598,  0.00336709,\n",
       "       -0.93620795,  0.37942395, -0.02589986,  0.38411242,  0.0466245 ,\n",
       "       -0.10019442,  0.1782563 ,  0.01617835, -0.50346518,  1.0576992 ,\n",
       "        0.57137579, -0.10598183,  0.65182644, -0.33499068, -0.62552601,\n",
       "       -1.68269169, -0.78732783,  0.14862084,  0.40488249, -0.27216882])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Adding Partitioning to the Index\n",
    "\n",
    "FAISS allows us to add an additional step to optimize our search efficiency using a variety of different methods. A popular approach is to partition the index into *[Voronoi cells](https://en.wikipedia.org/wiki/Voronoi_diagram)*. Using this method we would take our query vector `xq`, identify the *cell* it belongs to, and then use our `IndexFlatL2` to search between the query vector `xq` and all indexed vectors belonging to that *cell*. We can also include vectors from other nearby cells too.\n",
    "\n",
    "We initialize our new partitioned index by first adding our previous `IndexFlatL2` operation as a quantization step (another step in the search process), and feeding this into the new `IndexIVFFlat` operation like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlist = 50\n",
    "quantizer = faiss.IndexFlatL2(d)\n",
    "index = faiss.IndexIVFFlat(quantizer, d, nlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we've added a new parameter `nlist`. We use `nlist` to define how many partitions we'd like our index to have. \n",
    "\n",
    "When we built the previous, `IndexFlatL2`-only index, we noted that no training was required as no grouping/transformation was required to build that index. Now that we've added partitioning using `IndexIVFFlat`, this is no longer the case. Let's take a look at the `is_trained` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.is_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what we need to do now is `train` our index on our data, which we do *before* adding any data to the index (otherwise the index cannot know how to group each vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error in void __cdecl faiss::Clustering::train_encoded(__int64,const unsigned char *,const struct faiss::Index *,struct faiss::Index &,const float *) at D:\\a\\faiss-wheels\\faiss-wheels\\faiss\\faiss\\Clustering.cpp:281: Error: 'nx >= k' failed: Number of training points (10) should be at least as large as number of clusters (50)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m index\u001b[39m.\u001b[39;49mtrain(sentence_embeddings)\n\u001b[0;32m      2\u001b[0m index\u001b[39m.\u001b[39mis_trained\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\faiss\\class_wrappers.py:298\u001b[0m, in \u001b[0;36mhandle_Index.<locals>.replacement_train\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[39massert\u001b[39;00m d \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md\n\u001b[0;32m    297\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mascontiguousarray(x, dtype\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 298\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_c(n, swig_ptr(x))\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\faiss\\swigfaiss.py:5343\u001b[0m, in \u001b[0;36mIndexIVF.train\u001b[1;34m(self, n, x)\u001b[0m\n\u001b[0;32m   5341\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m, n, x):\n\u001b[0;32m   5342\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\" Trains the quantizer and calls train_residual to train sub-quantizers\"\"\"\u001b[39;00m\n\u001b[1;32m-> 5343\u001b[0m     \u001b[39mreturn\u001b[39;00m _swigfaiss\u001b[39m.\u001b[39;49mIndexIVF_train(\u001b[39mself\u001b[39;49m, n, x)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error in void __cdecl faiss::Clustering::train_encoded(__int64,const unsigned char *,const struct faiss::Index *,struct faiss::Index &,const float *) at D:\\a\\faiss-wheels\\faiss-wheels\\faiss\\faiss\\Clustering.cpp:281: Error: 'nx >= k' failed: Number of training points (10) should be at least as large as number of clusters (50)"
     ]
    }
   ],
   "source": [
    "index.train(sentence_embeddings)\n",
    "index.is_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our index is trained, we add our data just as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error in void __cdecl faiss::IndexIVFFlat::add_core(__int64,const float *,const __int64 *,const __int64 *) at D:\\a\\faiss-wheels\\faiss-wheels\\faiss\\faiss\\IndexIVFFlat.cpp:46: Error: 'is_trained' failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m index\u001b[39m.\u001b[39;49madd(sentence_embeddings)\n\u001b[0;32m      2\u001b[0m index\u001b[39m.\u001b[39mntotal\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\faiss\\class_wrappers.py:230\u001b[0m, in \u001b[0;36mhandle_Index.<locals>.replacement_add\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[39massert\u001b[39;00m d \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md\n\u001b[0;32m    229\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mascontiguousarray(x, dtype\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 230\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_c(n, swig_ptr(x))\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\faiss\\swigfaiss.py:5347\u001b[0m, in \u001b[0;36mIndexIVF.add\u001b[1;34m(self, n, x)\u001b[0m\n\u001b[0;32m   5345\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd\u001b[39m(\u001b[39mself\u001b[39m, n, x):\n\u001b[0;32m   5346\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\" Calls add_with_ids with NULL ids\"\"\"\u001b[39;00m\n\u001b[1;32m-> 5347\u001b[0m     \u001b[39mreturn\u001b[39;00m _swigfaiss\u001b[39m.\u001b[39;49mIndexIVF_add(\u001b[39mself\u001b[39;49m, n, x)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error in void __cdecl faiss::IndexIVFFlat::add_core(__int64,const float *,const __int64 *,const __int64 *) at D:\\a\\faiss-wheels\\faiss-wheels\\faiss\\faiss\\IndexIVFFlat.cpp:46: Error: 'is_trained' failed"
     ]
    }
   ],
   "source": [
    "index.add(sentence_embeddings)\n",
    "index.ntotal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's search again using the same indexed sentence embeddings and the same query `xq`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1 -1 -1 -1]]\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 12.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "D, I = index.search(xq, k)  # search\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can increase the number of nearby cells to search too with `nprobe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.nprobe = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "D, I = index.search(xq, k)  # search\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the number of `nprobe` will improve the accuracy of our search, but cost time. Our earlier `IndexFlatL2`-only search was *exhaustive* (it compared every single vector) and so it identified the closest matches with a perfect accuracy. The smaller our `nprobe` value, the smaller scope that we search. We received perfect results (that matched our previous `IndexFlatL2`-only results - `7460`, `10940`, `3781`, `5747`), however, if we found that we were not getting closely matching results, we could simply bump `nprobe` up further - improving accuracy, but increasing time-taken too.\n",
    "\n",
    "It's worth noting that the time taken can change with each run too, if we rerun the above block, we usually get a different time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "D, I = index.search(xq, k)\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For IVF (and IMI) indexes, before attempting to use the `reconstruct` method, we need to call the `make_direct_map` method - otherwise we will return a `RunetimeError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.reconstruct(7460)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `make_direct_map`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.make_direct_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.reconstruct(7460)[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now significantly reduced the search time, what can we do next?\n",
    "\n",
    "# Quantization\n",
    "\n",
    "Well, when storing these vectors we're storing the full (eg `Flat`) vector. Now in very big datasets this can quickly become a problem. Typically, we look at big datasets, and when working with large dataset we will find that storing the full vectors consumes too much space.\n",
    "\n",
    "Fortunately, FAISS comes with the ability to *compress* our vectors using transformations based on *Product Quantization* (PQ). But, what is PQ? Well, we can view it as an additional approximation step similar to our use of **IVF**, which allowed us to *approximate* by reducing the scope of our search. PQ is slightly different however, and approximates the distance (or similarity) calculation instead.\n",
    "\n",
    "[PQ explanation TODO REMOVE](https://mccormickml.com/2017/10/13/product-quantizer-tutorial-part-1/)\n",
    "\n",
    "PQ achieves this approximated distance operation by compressing the vectors themselves. This consists of a few steps.\n",
    "\n",
    "1. We split the every original vector into several subvectors.\n",
    "\n",
    "2. For each set of subvectors, we perform a clustering operation, creating many centroids for each subvector set.\n",
    "\n",
    "3. In our vector of subvectors, we replace each subvector with the ID of it's nearest centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 8  # number of centroid IDs in final compressed vectors\n",
    "bits = 8 # number of bits in each centroid\n",
    "\n",
    "quantizer = faiss.IndexFlatL2(d)  # we keep the same L2 distance flat index\n",
    "index = faiss.IndexIVFPQ(quantizer, d, nlist, m, bits) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we'll need to `train` the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.is_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.train(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And `add` our vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.add(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare it to our previous index *without* PQ, and an `nprobe` value of `10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.nprobe = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "D, I = index.search(xq, k)\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through adding PQ we've reduced our search time from ~7.5ms to ~5ms, a small difference on a dataset of this size, but when scaled to larger datasets this can make a huge difference.\n",
    "\n",
    "Now, we should also notice the slightly different results being returned. Beforehand with our exhaustive L2 search we were returning `7460`, `10940`, `3781`, and `5747`. Now, we see a slightly different order to our results - and two different vectors, `5013` and `5370`.\n",
    "\n",
    "Each of our speed optimization operations, `IVF` and `PQ`, come at the cost of accuracy. Now, if we print out these results we will nonetheless find that each item is still a relevant match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f'{i}: {sentences[i]}' for i in I[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So although we might not get the *perfect* result, we still get close - and thanks to the approximation, we get a significant speed boost"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a683edd788238e5c64f9fa2e4bdd4387776bc5c6f4f0a84da0685f9a25e421d6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('ml': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
